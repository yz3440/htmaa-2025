---
title: "HyperCarousel (V0)"
categories: ["final project", "weekly assignment"]
displayedCategory: "time for show"
date: "2025-12-15"
year: 2025
displayedDate: "Week 15"

description: "HyperCarousel is an add-on for the Kodak Carousel projectors and a novel slide mount design (HyperSlide) to add hypertext non-linear navigation capabilities to the slide projector."

shortDescription: "is Kodak Carousel an add-on to support hypertext non-linear navigation"

tags:
  - hypercarousel
  - MVP
  - prototype
  - mechanical
  - analog
  - computer vision

externalLink: null
videoLink: null

pressLinks: []

publicationLinks: []

priority: 2

roles: []

collaborators: []

image: "/project-assets/final-project/hypercarousel/hero.jpeg"
video: "/project-assets/final-project/hypercarousel/hero.mp4"

visible: true
featured: false

tableOfContents: true
---

<EmbeddedVideo src="https://vimeo.com/1147543474?share=copy&fl=sv&fe=ci" />

HyperCarousel is an add-on for the Kodak Carousel projectors and a novel slide mount design (HyperSlide) to add hypertext non-linear navigation capabilities to the slide projector.

## Origins

This project traces back to [Week 1](/projects/week-1-final-project-ideation/) and a DIY projector I built in Shanghai in 2021. The question: what if someone actually built Vannevar Bush's Memex using 1945 technology?

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/week-1/life-magazine-memex-screenshot.webp"
    caption="Bush's Memex concept (1945)"
    copyright="Life Magazine"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/week-1/film-projection-202104-1.jpeg"
    caption="My Shanghai prototype (2021)"
  />
</GridSection>

Week 1 me was idealistic: _"If I can do it purely mechanical, I wouldn't go electrical. If I have to go electrical, I wouldn't go electronic."_ Famous last words.

## System Architecture

Here's what I proposed in [Week 9](https://fab.cba.mit.edu/classes/863.25/people/YufengZhao/projects/week-9-hypercarousel-roadmap/):

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/system-diagram-old.png"
  caption="Week 9 plan: RPi Zero, mouse cursor projection, stepper motor, the works"
/>

And here's what I actually built:

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/system-diagram-new.png"
  caption="Week 15 reality: ESP32s, numpad, servo pressing buttons"
/>

### What Changed

| Aspect     | Plan                        | Reality                |
| ---------- | --------------------------- | ---------------------- |
| Controller | RPi Zero                    | ESP32S3                |
| Input      | PS/2 Mouse                  | PS/2 Numpad            |
| Cursor     | HDMI Projector overlay      | None                   |
| Motor      | Stepper + H-Bridge + Geneva | Servo pressing buttons |
| Power      | 12V + regulator             | 5V USB-C               |

**The cursor died first.** Turns out projectors can't focus on two slides at once — my overlay idea failed in 30 seconds of testing. No cursor means no mouse, no HDMI, no RPi Zero.

**The Geneva mechanism died next.** Instead of custom mechanics, I'm just pressing the Carousel's existing buttons with a servo arm. Dumb, but it works.

### So Much for Low-Tech

The _creator workflow_ stayed analog — printed transparencies, laser-cut cardstock, handwritten lookup tables. But the system itself? Multiple ESP32s, I2C, OCR. Firmly electronic.

The honest take: HyperCarousel is a digital device that enables an analog workflow. Not what I romanticized in Week 1, but it's the compromise that ships.

## HyperSlide Design

HyperSlide is a custom slide mount design that enables hypertext navigation in the Kodak Carousel. Each slide mount contains a lookup table that maps hyperlink numbers to destination slide numbers — essentially metadata attached to each slide.

### V0: Testing the Waters

I started by buying [Matin 35mm Slide Mounts 100 Pieces - Glassless White](https://www.amazon.com/dp/B008QT9AV2) to understand the physical constraints of slide mounts. I checked some references on slide film dimensions. According to ["How to Tell Which Type of Slides You Have"](https://kodakdigitizing.com/blogs/news/how-to-tell-which-type-of-slides-you-have) by Dillon Wallace, the 35mm film imaging size is 36mm × 24mm, but the entire film for one shot including the sprocket holes is 36mm × 35mm.

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-0-matin-slide-mount.png"
    caption="Matin plastic slide mounts — the starting point"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-1-film-size-diagram.png"
    caption="35mm film dimensions — imaging area vs. full frame"
  />
</GridSection>

For the transparency film, I bought [Caydo 8.5 x 11 Inch A4 Transparent Film for Screen Printing](https://www.amazon.com/dp/B0DFY8FWTX) (I got the A3 11×17 inch version, but they no longer sell that variant).

I'm most comfortable designing in Figma, but designing for print isn't a native feature. My workaround is to assign each pixel a real physical unit — in this design I'm using 1px as 1mm. So I have a 2794×4318 canvas and each film block is 360 × 350 (36mm × 35mm). When I export, I use 4× resolution to keep the text sharp, and when I print, I set the paper to 11×17 and scale to cover 100%.

The content of these slides is mostly sourced from Wikipedia. Text that's underlined indicates hyperlinks connecting to other slides.

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-2-slide-for-print-figma-design-screenshot.png"
    caption="Figma design — underlined text represents hyperlinks"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-3-slide-printed-on-film.jpeg"
    caption="Slides printed on inkjet transparency film"
  />
</GridSection>

Here's a slide about Center for Bits and Atoms mounted in a Matin mount, and a slide about Neil Gershenfeld projected through the Kodak Carousel 600:

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-4-slide-mounted.jpeg"
    caption="A mounted HyperSlide about CBA"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-5-slide-projected.jpeg"
    caption="Neil Gershenfeld slide projected through the Carousel"
  />
</GridSection>

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-99-figma-design-1x-export.jpg"
  caption="V0 complete design file"
/>

[Download V0 design (4× resolution)](/project-assets/final-project/hypercarousel/0-hyper-slide/v0-99-figma-design-4x-export.jpg)

In the original roadmap, I wanted to use a mouse/trackball interaction to control a cursor on the projector. So I did some testing — I drew a cursor on an empty piece of slide and put both the Neil slide and cursor slide into the projector. Turns out (obviously) the projector couldn't focus on both slides at the same time on the projection plane. This led to my following design that gives up on the cursor idea and settles on the numpad & lookup table compromise.

<Media
  src="/project-assets/final-project/hypercarousel/0-hyper-slide/v0-6-slide-cursor-dual-slide-test.mp4"
  caption="Testing dual slides for cursor overlay — spoiler: focus doesn't work that way"
/>

### V1: The Punchcard Inspiration

I was originally thinking I'd want to use a punchcard. This is because I want everything as analog as possible, and the IBM Punchcard was a well-established standard. I even found a variation with a slide embedded in a punchcard (taking up 1/3 of punch space, using the same size as IBM punchcard). These are called aperture cards, used for archiving image documents like blueprints efficiently with the help of sorting and indexing provided by the punch. Basically metadata-attached microfilms — something similar to what I want to achieve here.

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-0-ibm-punchcard.jpg"
    caption="IBM Punchcard — the original metadata storage"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-1-aperture-card.jpg"
    caption="Aperture card — film embedded in punchcard"
  />
</GridSection>

But then I'd need to use optical sensors to read it. I wasn't ready to die on this hill. I compromised by allowing myself to use a camera system via ESP32, so I designed a human-readable table with some marks for camera registration. A person can just write the lookup table (which link goes to which slide, or a hyperlink index → slide number mapping) on the slide mount.

So this is my design: 5 mounts laid out on a letter-sized canvas. I made the width the same as the slide mount requirement (2 inch / 51.3mm) and doubled the height to 4 inch (102.6mm). I located the 36×24mm film cutout at the right location and put a two-column table on top (one says `link #`, the other says `to #`).

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-2-figma-design-letter-size.jpg"
  caption="V1 design layout — 5 HyperSlide mounts on letter-sized cardstock"
/>

I used [White Cardstock 8.5 x 11, 230gsm Cover Cardstock Paper, 85 Lb Heavy Card](https://www.amazon.com/dp/B0CJX2153R) for the material.

I put my design in the xTool P2 laser cutter. I wanted to engrave the table and cut the mount contour and film slot. I realized the text part is recognized as engrave, and all the rest are score. I needed to manually assign the text to engrave, the table frame lines to score, and the outer lines to cut.

Also, my SVG did not preserve dimensions well. I had to manually rescale the shapes so that the width of each one is 50mm.

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-3-xTool-setting.jpeg"
  caption="xTool Creative Space settings for the laser cut"
/>

| Operation | Power | Speed   |
| --------- | ----- | ------- |
| Cut       | 80%   | 70mm/s  |
| Score     | 4%    | 160mm/s |
| Engrave   | 4%    | 225mm/s |

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-4-card-results-in-cutter.jpeg"
    caption="Five HyperSlide mounts fresh off the laser cutter"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-5-hyperslide-size-comparision-with-matin.jpeg"
    caption="HyperSlide (left) vs. Matin mount (right)"
  />
</GridSection>

I taped my film and the slide mount together:

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-6-hyperslide-taped.jpeg"
    caption="Film transparency taped to cardstock mount"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-8-measure-matin.jpeg"
    caption="Measuring Matin mount with caliper — 50.15mm"
  />
</GridSection>

I tested the film in my projector... and it got stuck halfway. I think it might be too wide? I used my caliper to measure the Matin slide mount, and it reads 50.15mm — same as what I had. But paper flexes more than plastic. So I decided to trim my cardstock on both sides with scissors and test again. It worked! The slide goes up and down in the projector smoothly.

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-7-test-stuck.mp4"
    caption="First test — slide gets stuck halfway"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v1-8-test-smooth-trimmed.mp4"
    caption="After trimming — smooth sliding!"
  />
</GridSection>

[Download V1 design SVG](/project-assets/final-project/hypercarousel/0-hyper-slide/v1-0-figma-design-export.svg) (don't use this though — the width is weird)

### V2: Getting the Width Right

I changed the width from 50mm to 48mm and everything works now. I used the same laser settings and made ten of them on two pieces of letter-sized cardstock. And I used glue stick this time to mount the film — a lot cleaner!

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v2-1-xTool-toolpath-preview.jpeg"
    caption="xTool toolpath preview for batch cutting"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/0-hyper-slide/v2-3-gluestick-mount.jpeg"
    caption="Glue stick mounting — much cleaner than tape"
  />
</GridSection>

Here's a 10× speed timelapse of the cutting process:

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/0-hyper-slide/v2-2-cutting-timelapse.mp4"
  caption="Timelapse of laser cutting 10 HyperSlide mounts"
/>

[Download V2 design SVG](/project-assets/final-project/hypercarousel/0-hyper-slide/v2-0-figma-export.svg)

### V3: Seven-Segment Marking Attempt

Later in the project I was having trouble with digit recognition, so I decided to change the design to include some marking to help users write seven-segment digits at designated locations.

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/0-hyper-slide/v3-0-film-mounts.jpeg"
  caption="V3 design with seven-segment digit guides scored on the mount"
/>

I did not end up using these because the scoring marks are too dark even at 200mm/s and 1% power setting, which creates high contrast and messes with my CV algorithm even more. I'll talk about this in detail in the lookup-table recognition section below.

[Download V3 design SVG](/project-assets/final-project/hypercarousel/0-hyper-slide/v3-1-figma-export.svg)

## Aluminum Frame

I started out wanting to prototype with 2020 aluminum extrusions — they're easy to work with and let you iterate on mounting configurations quickly. I arranged four pieces in a square: the front and back rails sit at the bottom, while the left and right rails are raised higher. This creates a cradle that locks the projector in place from all sides.

Turns out the Kodak Carousel 600 fits _perfectly_ in a 276×276mm square. Like, suspiciously perfectly. You can even lift the whole assembly up by the frame without worrying about the projector falling out — the fit is that snug. What started as a quick test jig ended up being good enough for the final presentation, so I just kept it.

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/1-aluminum-frame/0-testing-dimensions-up-side-down.jpeg"
    caption="Testing dimensions — projector upside down in the frame"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/1-aluminum-frame/1-testing-dimensions.jpeg"
    caption="Perfect fit — 276×276mm square cradles the Carousel"
  />
</GridSection>

## Button Pressor Servo

The Kodak Carousel has physical buttons for advancing slides forward and backward. Rather than hacking into the electronics, I decided to just... press the buttons with a servo. Sometimes the dumbest solution is the best solution.

The servo I'm using is an [ANNIMOS 20KG Digital Servo](https://www.amazon.com/ANNIMOS-Digital-Waterproof-DS3218MG-Control/dp/B076CNKQX4) — high torque, full metal gear, waterproof (overkill for button pressing, but I had it lying around). It should be more than enough to press the button.

### V0: Initial Design

I planned to mount the servo on the aluminum extrusion near the film advancement button. First step: measure the servo. The body (not including the mounting wings) is 39.67mm × 19.88mm. The mounting screws are M3, so I'm using 3.4mm holes.

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/v0-0-measuring-servo.jpeg"
  caption="Measuring the servo body — 39.67mm × 19.88mm"
/>

I modeled everything parametrically in Fusion 360 using 40 × 20mm as the nominal dimensions. The bottom is shaped to sit on top of the extrusion while leaving clearance for the projector body (which sticks out a bit on the sides).

<GLBViewer
  width="66%"
  src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/v0.glb"
  caption="V0 servo holder design"
  cameraPosition={[80, -120, 200]}
  backgroundColor="gray"
/>

[Download V0 STL](/project-assets/final-project/hypercarousel/2-button-pressor-servo/v0.stl)

I forgot to add a notch for the cable coming out from the motor. Back to Fusion 360...

### V1: Cable Notch

Added the notch for the cable, plus some curves on the edges hoping it would make them thicker for better structural integrity.

<GridSection columnCount={2}>
  <GLBViewer
    width="100%"
    src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/v1.glb"
    caption="V1 with cable notch"
    cameraPosition={[80, -10, 200]}
    objectPosition={[0, -50, 0]}
    backgroundColor="gray"

/>

  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/v1-works.jpeg"
    caption="V1 fit test — it works!"
  />
</GridSection>

[Download V1 STL](/project-assets/final-project/hypercarousel/2-button-pressor-servo/v1.stl)

It works great, but I think it would look nicer if the model had recesses where the M3 nuts go.

### V2: Nut Recesses

Added hexagonal pockets for the nuts to sit flush. All the dimensions are parameterized in the Fusion 360 file so they're easy to tweak.

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/v2-design-with parameters.jpeg"
    caption="Fusion 360 parameters — everything is adjustable"
  />
  <GLBViewer
    width="100%"
    src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/v2.glb"
    caption="V2 with nut recesses"
    cameraPosition={[80, -10, 200]}
    objectPosition={[0, -50, 0]}
    backgroundColor="gray"

/>

</GridSection>

The previous prints were black PLA. For the final version, I switched to [OVERTURE Rock PLA Marble filament](https://www.amazon.com/dp/B0F4RCJ659) — it has mixed particles that give a white pebble stone texture. Bonus: it hides layer lines really well.

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/v2-test-fit-on-aluminum-extrusion.jpeg"
  caption="V2 test fit on aluminum extrusion — clean marble finish"
/>

[Download V2 STL](/project-assets/final-project/hypercarousel/2-button-pressor-servo/v2.stl) | [Download Fusion 360 file](/project-assets/final-project/hypercarousel/2-button-pressor-servo/button-pressor-servo-holder.f3d)

### Software Test

Before integrating with the main board, I wrote a standalone test program to verify the servo works. It runs on an ESP32 (XIAO ESP32-S3) and serves a web interface for controlling the servo — useful for finding the right angles without recompiling. I tested this on the MLDEV WiFi network at Media Lab.

The code sets up a simple web server with two main features: a slider for direct angle control, and a sequence player that can run through a list of angles with configurable delays between each step. The sequence format is just `angle,delay_ms` per line, which made it easy to test different button-pressing motions.

Key configuration — the servo rests at 120° and swings down to 0° to press the button:

```cpp
// Servo positions and speed
#define RESTING_POSITION 120    // Resting position (degrees)
#define ACTIVATION_POSITION 0   // Activation position (degrees)
#define SERVO_SPEED_MS 800      // Time to move from 0 to 120 degrees (ms)
#define SERVO_MS_PER_DEGREE (SERVO_SPEED_MS / 120.0)  // ~6.67ms per degree
```

The sequence execution runs in the main loop, checking if enough time has passed before moving to the next step:

```cpp
// Handle sequence execution
if (sequenceRunning && stepCount > 0) {
  unsigned long now = millis();
  if (now - lastSequenceTime >= steps[sequenceIndex].delayMs) {
    sequenceIndex++;
    if (sequenceIndex >= stepCount) {
      // Sequence complete
      sequenceRunning = false;
      sequenceIndex = 0;
    } else {
      // Execute next step
      myServo.write(steps[sequenceIndex].angle);
      currentAngle = steps[sequenceIndex].angle;
      lastSequenceTime = now;
    }
  }
}
```

[Download servo-test.cpp](/project-assets/final-project/hypercarousel/2-button-pressor-servo/servo-test.cpp)

<Media
  src="/project-assets/final-project/hypercarousel/2-button-pressor-servo/software-test-work.mp4"
  caption="Servo test via web interface"
/>

You'll notice some weird text in the web interface — the frontend was vibe-coded with emoji, but the ESP32 doesn't render them properly. It still works though!

## Circuit Design

Back to the system diagram — everything connects through an I2C bus using JST-XH 4-pin connectors. This design extends what I built in [Week 12](/projects/week-12-ps2-numpad-via-i2c-bridge/), but with a few critical additions.

<Media
  widthPercentage={66}
  src="/project-assets/final-project/hypercarousel/system-diagram-new.png"
  caption="System architecture — I2C bus connecting all modules"
/>

### The 5V vs 3.3V Problem

Most of my peripherals — the PS/2 numpad submodule and the ESP32S3 camera — run happily on 5V I2C. But the Hall effect rotary encoder I'm using requires 3.3V. I couldn't just daisy-chain everything on a single bus voltage.

The solution: two separate I2C buses with different power rails. Same SDA/SCL lines from the ESP32S3, but different JST connectors with different VCC voltages.

| Connector Type | VCC  | Pull-up | Connected Devices              |
| -------------- | ---- | ------- | ------------------------------ |
| 5V I2C (×4)    | 5V   | 4.99kΩ  | PS/2 submodule, Camera ESP32S3 |
| 3.3V I2C (×1)  | 3.3V | 4.99kΩ  | Hall effect sensor             |

I don't actually need four 5V I2C connectors — I only have two devices on that bus. But my [Week 12](/projects/week-12-ps2-numpad-via-i2c-bridge/) main board design already had four, and more connectors means more flexibility for future expansion. No reason to remove them.

### Main Board Design

The main board is built around a XIAO ESP32S3 and includes:

- **4× 5V 4-pin JST-XH I2C connectors** — standard I2C with 5V power for most peripherals
- **1× 3.3V 4-pin JST-XH I2C connector** — dedicated bus for the Hall effect sensor
- **1× 3-pin servo connector** — PWM signal on a GPIO pin, plus 5V and GND
- **1× bistable switch** — connected to a digital GPIO and ground, for mode selection

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/3-circuit-design/mainboard-schematic.png"
    caption="Main board schematic — note the separate 5V and 3.3V I2C connectors"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/3-circuit-design/mainboard-pcb.png"
    caption="Main board PCB layout"
  />
</GridSection>

{/* TODO: Add assembled board photo */}

The pull-up resistors are 4.99kΩ on both buses, which is a reasonable middle ground for the cable lengths I'm dealing with (under 30cm for most connections).

### PS/2 Daughter Board

The PS/2 numpad submodule is exactly what I built in [Week 12](https://fab.cba.mit.edu/classes/863.25/people/YufengZhao/projects/week-12-ps2-numpad-via-i2c-bridge/). I didn't change the code at all — it just works. The submodule has its own ESP32-C3 that reads PS/2 keystrokes and exposes them over I2C at address `0x08`. The main board polls it periodically to check for new keypresses.

### Camera Module

For the camera, I'm using an ESP32S3 Sense — the variant that comes with a detachable OV2640 camera module. This board also sits on the I2C bus for communication with the main controller, though the actual image processing happens on-board. I designed a simple breakout board for it:

<GridSection columnCount={2}>
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/3-circuit-design/esp32s3-cam-schematic.png"
    caption="ESP32S3 camera breakout schematic"
  />
  <Media
    widthPercentage={100}
    src="/project-assets/final-project/hypercarousel/3-circuit-design/esp32s3-cam-pcb.png"
    caption="ESP32S3 camera breakout PCB"
  />
</GridSection>

More on the camera software in the Lookup Table Recognition section.

## Lookup Table Recognition

The whole point of the HyperSlide design is that each slide mount has a handwritten lookup table mapping hyperlink numbers to destination slide numbers. The camera's job is to read this table when a slide is ejected.

### Camera Mounting

I added a new vertical aluminum extrusion bar to the frame specifically for the camera. The bar sits perpendicular to the projector's slide ejection slot, giving the camera a top-down view of the slide mount.

{/* TODO: Add photo of camera on aluminum extrusion */}

### Camera Holder Design

The 3D printed holder has elongated M3 screw slots instead of fixed holes. This lets me slide the camera up and down to find the optimal height and viewing angle before tightening the screws. Small adjustment range, but it makes a huge difference when trying to get consistent framing.

{/* TODO: Add GLBViewer for camera holder */}
{/* TODO: Add photo of camera holder */}

### The CV Pipeline (In Theory)

The recognition pipeline should work like this:

1. **Capture frame** from ESP32S3 camera when slide ejection is detected
2. **Quad detection** to find the four corners of the lookup table area
3. **Perspective correction** (homography) to get a flat, rectangular view
4. **Digit segmentation** to isolate each number in the table
5. **Digit recognition** (MNIST-style classification) to read the values

I built a web interface to preview the ESP32S3 camera feed and visualize each processing step. The ESP32S3 serves a simple webpage that shows the raw camera image and overlays the detected features.

```cpp
// ESP32S3 camera web server - serves MJPEG stream and processing preview
#include "esp_camera.h"
#include <WiFi.h>
#include <WebServer.h>

WebServer server(80);

void handleStream() {
  WiFiClient client = server.client();
  camera_fb_t *fb = esp_camera_fb_get();
  if (!fb) {
    Serial.println("Camera capture failed");
    return;
  }

  // Send MJPEG frame
  client.println("HTTP/1.1 200 OK");
  client.println("Content-Type: multipart/x-mixed-replace; boundary=frame");
  client.println();

  while (client.connected()) {
    fb = esp_camera_fb_get();
    if (fb) {
      client.printf("--frame\r\nContent-Type: image/jpeg\r\nContent-Length: %d\r\n\r\n", fb->len);
      client.write(fb->buf, fb->len);
      client.println();
      esp_camera_fb_return(fb);
    }
    delay(30);
  }
}

void setup() {
  // Camera init, WiFi connect, etc.
  server.on("/stream", handleStream);
  server.begin();
}
```

{/* TODO: Add screenshot of web interface */}

### The Reality

Here's where it gets frustrating. The Carousel ejects slides at slightly different angles every time — sometimes tilted left, sometimes right, sometimes perfectly straight. This inconsistency throws off my quad detection, which in turn ruins the perspective correction.

I tried using the V3 HyperSlide design with seven-segment digit guides scored into the cardstock. The idea was to help users write consistent, recognizable digits. But the laser scoring created too much contrast — the dark score lines confused my CV algorithm more than the handwritten digits themselves.

### Google Vision API Attempt

In desperation, I deployed a simple web service on Vercel that proxies images to Google Cloud Vision API for OCR:

```typescript
// Vercel serverless function for Google Vision OCR
import { ImageAnnotatorClient } from "@google-cloud/vision";

export default async function handler(req, res) {
  const client = new ImageAnnotatorClient();
  const imageBuffer = Buffer.from(req.body.image, "base64");

  const [result] = await client.textDetection({
    image: { content: imageBuffer },
  });

  const detections = result.textAnnotations;
  res.json({ text: detections?.[0]?.description || "" });
}
```

Results: still terrible. Google Vision is optimized for printed text and clean documents, not handwritten digits on textured cardstock photographed through a fisheye ESP32 camera lens.

### What I Think I Need

Better markers. The current design relies on finding the lookup table boundaries visually, but there's nothing for the camera to reliably lock onto. I think I need:

1. **High-contrast ArUco markers** or similar fiducials in fixed positions on the slide mount
2. **Better lighting** — the projector lamp creates harsh shadows that vary with slide position
3. **Higher resolution camera** — the OV2640 maxes out at 2MP, which might not be enough for small handwritten digits

This part of the project is still very much in progress.

## Rotary Encoder

To track which slide is currently in position, I'm using a Hall effect sensor to detect the carousel tray rotation.

### Why Hall Effect

The Kodak Carousel tray has 80 slots arranged in a circle. As the tray rotates, I need to count how many slots have passed to know which slide is currently at the projection position. A Hall effect sensor can detect the metal parts of the tray mechanism as they pass by.

### Sensor Design

I mounted the Hall effect sensor on a small bracket attached to the aluminum frame. The sensor sits about 3-4mm away from the rotating tray mechanism — close enough to detect reliably, far enough to avoid physical contact.

{/* TODO: Add photo of Hall effect sensor mounted */}
{/* TODO: Add diagram of sensor position */}

The tolerance is a few millimeters, which should be fine given the carousel's mechanical consistency. In isolated testing, the sensor reliably detects each slot transition.

### The I2C Problem

Here's where things went wrong. The Hall effect sensor I'm using has an I2C interface (hence the dedicated 3.3V I2C connector on the main board). When I test it alone — just the sensor connected to the ESP32S3 — it works perfectly. Clean signals, reliable readings, no issues.

But the moment I connect other I2C devices (the PS/2 submodule, the camera board), the bus starts throwing errors. The main board gets stuck waiting for I2C responses, the sensor readings become garbage, and sometimes the whole bus locks up.

I've tried:

- **Different pull-up values** — no change
- **Shorter cables** — marginal improvement
- **Checking for address conflicts** — all devices have unique addresses
- **Adding bus recovery logic** — helps sometimes, but not reliably

I don't know exactly why this happens. My best guesses:

- **Capacitance issues** from multiple devices loading the bus
- **Timing conflicts** between fast and slow I2C devices
- **Power supply noise** affecting the 3.3V rail specifically

For now, the Hall effect sensor is disconnected while I debug. The system works fine without position tracking — I just have to manually keep track of which slide I'm on, which defeats some of the purpose but at least lets me demo the core hyperlink functionality.

{/* TODO: Add screenshot of I2C error if available */}
